{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import mobilenetv3, ResNet50_Weights\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import (\n",
    "    lraspp_mobilenet_v3_large,\n",
    "    deeplabv3_resnet50,\n",
    "    deeplabv3_mobilenet_v3_large,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ImageClassification(\n",
       "     crop_size=[224]\n",
       "     resize_size=[256]\n",
       "     mean=[0.485, 0.456, 0.406]\n",
       "     std=[0.229, 0.224, 0.225]\n",
       "     interpolation=InterpolationMode.BILINEAR\n",
       " ),\n",
       " ImageClassification(\n",
       "     crop_size=[224]\n",
       "     resize_size=[256]\n",
       "     mean=[0.485, 0.456, 0.406]\n",
       "     std=[0.229, 0.224, 0.225]\n",
       "     interpolation=InterpolationMode.BILINEAR\n",
       " ))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_resnet = ResNet50_Weights.IMAGENET1K_V1.transforms()\n",
    "trans_mobilenet = mobilenetv3.MobileNet_V3_Large_Weights.IMAGENET1K_V1.transforms()\n",
    "trans_resnet, trans_mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[224]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"segtrain_data/train/000000.npy\"\n",
    "with open(fp, \"rb\") as f:\n",
    "    npzfile = np.load(f)\n",
    "    input_data = npzfile[\"input\"]  # (4, 256, 256) (C,H,W)\n",
    "    output_data = npzfile[\"output\"]  # (1, 256, 256) (C,H,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 23)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(input_data), np.min(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdaroya_umass_edu/miniconda3/envs/lsa/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.9428), tensor(-1.6042))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.from_numpy(input_data[:3, :, :]/255.)\n",
    "tmp2 = trans(tmp)\n",
    "torch.max(tmp2), torch.min(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 256, 256), 220, 30, array([63.35630798, 77.28466797, 68.68429565]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = input_data[:3, :, :]\n",
    "# x = (x-np.mean(x, axis=(1,2)).reshape(-1,1,1))/np.var(x, axis=(1,2)).reshape(-1,1,1)\n",
    "x.shape, np.max(x), np.min(x), np.mean(x, axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdaroya_umass_edu/miniconda3/envs/lsa/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.9428), tensor(-1.6042))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.from_numpy(input_data[:3, :, :])\n",
    "tmp2 = trans(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataRGB(Dataset):\n",
    "    \"\"\"\n",
    "    This file is directly modified from https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "    \"\"\"\n",
    "    def __init__(self, root, split='train', transforms=None):\n",
    "        self.split = split\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # R\\read the data file\n",
    "        if split==\"train\":\n",
    "            self.data_path = os.path.join(root, 'train')\n",
    "        elif split==\"val\":\n",
    "            self.data_path = os.path.join(root, 'val')\n",
    "        elif split==\"test\":\n",
    "            self.data_path = os.path.join(root, 'test')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # calculate data length\n",
    "        self.fps = fnmatch.filter(os.listdir(self.data_path), '*.npy')\n",
    "        self.data_len = len(self.fps)\n",
    "        # self.data_len = 20\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # fp = os.path.join(self.data_path, f\"{index:06d}.npy\")\n",
    "        fp = os.path.join(self.data_path, self.fps[index])\n",
    "        with open(fp, \"rb\") as f:\n",
    "            npzfile = np.load(f)\n",
    "            input_data = npzfile[\"input\"]  # (4, 256, 256) (C,H,W)\n",
    "            output_data = npzfile[\"output\"]  # (1, 256, 256) (C,H,W)\n",
    "        \n",
    "        image = torch.from_numpy(input_data[:3,:,:])\n",
    "        label = torch.from_numpy(output_data)\n",
    "        if self.transforms is not None:\n",
    "            crop_size = self.transforms.__dict__[\"crop_size\"][0]\n",
    "            image = image[:, :crop_size, :crop_size]\n",
    "            label = label[:, :crop_size, :crop_size]\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        return (\n",
    "            image.type(torch.FloatTensor),\n",
    "            label.type(torch.FloatTensor),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = trans\n",
    "train_dataset = SatelliteDataRGB(root=\"segtrain_data\", split=\"train\", transforms=trans)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdaroya_umass_edu/miniconda3/envs/lsa/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 224, 224]) torch.Size([12, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
